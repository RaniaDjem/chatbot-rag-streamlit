import streamlit as st
import sys
import tempfile
from utils import process_pdf, get_chain

# Ignore the torch module from being watched by Streamlit to avoid path issues
sys.modules["torch.classes"] = None

st.set_page_config(page_title="Chatbot RAG PDF", page_icon="ğŸ§ ")

# ğŸ“ Introduction
st.title("ğŸ“„ Chatbot Intelligent sur Document PDF")
st.markdown("""
Bienvenue sur **Chatbot RAG PDF** ! ğŸš€

TÃ©lÃ©versez simplement un fichier **PDF** (article, rapport, cours, etc.), et posez **toutes vos questions** dessus âœ¨  
Notre assistant intelligent lit le document et vous rÃ©pond prÃ©cisÃ©ment, en s'appuyant sur le contenu du PDF.

_Câ€™est comme avoir un assistant personnel qui lit pour vous et vous rÃ©sume tout_ ğŸ¤–ğŸ“„
""")

# ğŸ“ Fichier PDF
uploaded_file = st.file_uploader("Choisissez un fichier PDF", type="pdf")

if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        pdf_path = tmp_file.name

    st.success("Document chargÃ©. Initialisation en cours...")
    vectorstore = process_pdf(pdf_path)
    qa_chain = get_chain(vectorstore)

    st.success("Chatbot prÃªt ! Posez votre question.")
    question = st.text_input("Votre question :")

    if question:
        with st.spinner("GÃ©nÃ©ration de la rÃ©ponse..."):
            response = qa_chain.run(question)
            st.markdown(f"**RÃ©ponse :** {response}")

# ğŸ› ï¸ Stack technique & crÃ©dits
st.markdown("""
---
ğŸ› ï¸ **Stack technique** utilisÃ©e :
- [Streamlit](https://streamlit.io/) pour l'interface web
- [LangChain](https://www.langchain.com/) pour orchestrer le RAG (Retrieval-Augmented Generation)
- [Mistral AI](https://mistral.ai/) pour le modÃ¨le de langage (LLM)
- [Hugging Face Transformers](https://huggingface.co/) pour les embeddings
- [FAISS](https://github.com/facebookresearch/faiss) pour la recherche sÃ©mantique

ğŸ’¡ Ce projet a Ã©tÃ© rÃ©alisÃ© par **Rania Djema**.
""")
